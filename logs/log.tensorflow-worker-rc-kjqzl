2018-04-24 01:28:19.247457: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> 10.244.192.22:2222}
2018-04-24 01:28:19.247559: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> 10.244.64.16:2222, 1 -> localhost:2222}
2018-04-24 01:28:19.249351: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:333] Started server with target: grpc://localhost:2222
WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.
Instructions for updating:
Use the retry module or similar alternatives.
WARNING:tensorflow:From dense_classifier.py:179: streaming_auc (from tensorflow.contrib.metrics.python.ops.metric_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.metrics.auc. Note that the order of the labels and predictions arguments has been switched.
WARNING:tensorflow:From dense_classifier.py:207: __init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.MonitoredTrainingSession
2018-04-24 01:28:32.644398: I tensorflow/core/distributed_runtime/master_session.cc:1033] Start master session e8abf315bff972a3 with config: 
Step: 0, loss: 20.5749950409, accuracy: 0.556640625, auc: 0.525639295578
Step: 30, loss: 1.0735013485, accuracy: 0.890625, auc: 0.6618475914
Step: 40, loss: 0.730256795883, accuracy: 0.8759765625, auc: 0.703047573566
Step: 70, loss: 0.411189705133, accuracy: 0.9580078125, auc: 0.761229157448
Step: 80, loss: 0.318770766258, accuracy: 0.96484375, auc: 0.782132029533
Step: 100, loss: 0.275798946619, accuracy: 0.9677734375, auc: 0.811454534531
Step: 110, loss: 0.221546977758, accuracy: 0.9619140625, auc: 0.823762834072
Step: 130, loss: 0.172754481435, accuracy: 0.9541015625, auc: 0.844407856464
Step: 150, loss: 0.142737776041, accuracy: 0.9462890625, auc: 0.859600484371
Step: 160, loss: 0.120730981231, accuracy: 0.95703125, auc: 0.865748047829
Step: 170, loss: 0.113922715187, accuracy: 0.953125, auc: 0.8708999753
Step: 190, loss: 0.0884228944778, accuracy: 0.94921875, auc: 0.879321694374
Step: 210, loss: 0.0910380482674, accuracy: 0.953125, auc: 0.890376150608
Step: 220, loss: 0.0885283201933, accuracy: 0.9541015625, auc: 0.89329713583
Step: 240, loss: 0.0850980505347, accuracy: 0.9521484375, auc: 0.898960411549
Step: 250, loss: 0.0870769023895, accuracy: 0.958984375, auc: 0.901567518711
Step: 260, loss: 0.0741936713457, accuracy: 0.94921875, auc: 0.903725504875
Step: 280, loss: 0.0802343860269, accuracy: 0.96484375, auc: 0.907981932163
Step: 300, loss: 0.0723102092743, accuracy: 0.958984375, auc: 0.911831974983
Step: 320, loss: 0.0650369375944, accuracy: 0.9609375, auc: 0.914950013161
Step: 340, loss: 0.0797435045242, accuracy: 0.962890625, auc: 0.917947351933
Step: 350, loss: 0.0649936050177, accuracy: 0.9580078125, auc: 0.919294595718
Step: 360, loss: 0.0671360641718, accuracy: 0.9580078125, auc: 0.920529544353
Step: 380, loss: 0.0643498897552, accuracy: 0.9638671875, auc: 0.923070073128
Step: 400, loss: 0.0740272700787, accuracy: 0.9677734375, auc: 0.925553441048
Step: 410, loss: 0.0643286034465, accuracy: 0.966796875, auc: 0.927575588226
Step: 420, loss: 0.0634007006884, accuracy: 0.9658203125, auc: 0.928502976894
Step: 430, loss: 0.0686370506883, accuracy: 0.9580078125, auc: 0.929421842098
Step: 450, loss: 0.0622732527554, accuracy: 0.9609375, auc: 0.931011080742
Step: 460, loss: 0.0640475004911, accuracy: 0.9677734375, auc: 0.931823909283
Step: 470, loss: 0.0536397919059, accuracy: 0.966796875, auc: 0.932545125484
Step: 490, loss: 0.0596705749631, accuracy: 0.96875, auc: 0.93418854475
Step: 510, loss: 0.0571414195001, accuracy: 0.966796875, auc: 0.935604095459
Step: 530, loss: 0.0542453825474, accuracy: 0.962890625, auc: 0.936752438545
Step: 560, loss: 0.0492473766208, accuracy: 0.9697265625, auc: 0.938551902771
Step: 570, loss: 0.0565024986863, accuracy: 0.9658203125, auc: 0.939136147499
Step: 590, loss: 0.0509423539042, accuracy: 0.966796875, auc: 0.940272212029
Step: 600, loss: 0.0545282997191, accuracy: 0.96875, auc: 0.940818488598
Step: 620, loss: 0.0530777797103, accuracy: 0.966796875, auc: 0.942265927792
Step: 630, loss: 0.0534795895219, accuracy: 0.966796875, auc: 0.942709684372
Step: 650, loss: 0.0535629838705, accuracy: 0.9658203125, auc: 0.943482339382
Step: 660, loss: 0.0538560561836, accuracy: 0.966796875, auc: 0.943873047829
Step: 670, loss: 0.0536853410304, accuracy: 0.96484375, auc: 0.944243609905
Step: 690, loss: 0.0482473149896, accuracy: 0.970703125, auc: 0.94504570961
Step: 700, loss: 0.0466464199126, accuracy: 0.966796875, auc: 0.94578063488
Step: 710, loss: 0.0526707135141, accuracy: 0.9658203125, auc: 0.946073293686
Step: 730, loss: 0.0499132871628, accuracy: 0.9716796875, auc: 0.946739137173
Step: 750, loss: 0.0505192130804, accuracy: 0.96875, auc: 0.947349607944
Step: 760, loss: 0.0530971102417, accuracy: 0.9658203125, auc: 0.947639584541
Step: 790, loss: 0.0508493036032, accuracy: 0.9658203125, auc: 0.948450803757
Step: 800, loss: 0.0453484468162, accuracy: 0.966796875, auc: 0.948741734028
Step: 810, loss: 0.048721652478, accuracy: 0.966796875, auc: 0.949275255203
Step: 830, loss: 0.0546580664814, accuracy: 0.9677734375, auc: 0.949811160564
Step: 840, loss: 0.0453959330916, accuracy: 0.96484375, auc: 0.950029790401
Step: 850, loss: 0.05326455459, accuracy: 0.974609375, auc: 0.950306773186
Step: 860, loss: 0.0532672852278, accuracy: 0.9677734375, auc: 0.950543344021
Step: 870, loss: 0.0512324050069, accuracy: 0.9658203125, auc: 0.950732707977
Step: 880, loss: 0.04840233922, accuracy: 0.966796875, auc: 0.950886666775
Step: 900, loss: 0.0442762188613, accuracy: 0.9638671875, auc: 0.951290845871
Step: 910, loss: 0.0468600690365, accuracy: 0.9677734375, auc: 0.951496481895
Step: 940, loss: 0.0482665225863, accuracy: 0.966796875, auc: 0.952019631863
Step: 950, loss: 0.0488350987434, accuracy: 0.9677734375, auc: 0.952242970467
Step: 960, loss: 0.0419471338391, accuracy: 0.9677734375, auc: 0.952432692051
Step: 990, loss: 0.0439604669809, accuracy: 0.96875, auc: 0.952978551388
Step: 1010, loss: 0.0542281866074, accuracy: 0.9638671875, auc: 0.953446149826
Step: 1020, loss: 0.044650927186, accuracy: 0.9658203125, auc: 0.95375585556
Step: 1030, loss: 0.046525105834, accuracy: 0.970703125, auc: 0.953901946545
Step: 1040, loss: 0.0442330874503, accuracy: 0.9677734375, auc: 0.954071581364
Step: 1050, loss: 0.0469472631812, accuracy: 0.953125, auc: 0.954189896584
Step: 1060, loss: 0.0441710427403, accuracy: 0.970703125, auc: 0.954367637634
Step: 1070, loss: 0.0495199300349, accuracy: 0.966796875, auc: 0.954527020454
Step: 1080, loss: 0.039844237268, accuracy: 0.9658203125, auc: 0.954641461372
Step: 1090, loss: 0.0439567267895, accuracy: 0.9677734375, auc: 0.954791843891
Step: 1100, loss: 0.0452484115958, accuracy: 0.966796875, auc: 0.954916954041
Step: 1110, loss: 0.0414891168475, accuracy: 0.9677734375, auc: 0.955080509186
Step: 1120, loss: 0.0441046506166, accuracy: 0.9677734375, auc: 0.955228567123
Step: 1130, loss: 0.0479298681021, accuracy: 0.96484375, auc: 0.955481052399
Step: 1140, loss: 0.0419564172626, accuracy: 0.9638671875, auc: 0.955591261387
Step: 1150, loss: 0.0411911793053, accuracy: 0.9697265625, auc: 0.955749809742
Step: 1160, loss: 0.0437084771693, accuracy: 0.9677734375, auc: 0.955872893333
Step: 1170, loss: 0.0393254421651, accuracy: 0.966796875, auc: 0.956005275249
Step: 1180, loss: 0.0419008508325, accuracy: 0.96484375, auc: 0.956125438213
Step: 1190, loss: 0.0458488166332, accuracy: 0.966796875, auc: 0.956228792667
Step: 1200, loss: 0.0423214584589, accuracy: 0.96875, auc: 0.956376194954
Step: 1210, loss: 0.041187055409, accuracy: 0.96484375, auc: 0.956568598747
Step: 1250, loss: 0.0414640679955, accuracy: 0.9599609375, auc: 0.956893801689
Step: 1260, loss: 0.0431491062045, accuracy: 0.9697265625, auc: 0.957027077675
Step: 1270, loss: 0.0413931161165, accuracy: 0.96484375, auc: 0.957070171833
Step: 1280, loss: 0.0417789742351, accuracy: 0.966796875, auc: 0.957180202007
Step: 1310, loss: 0.0396052747965, accuracy: 0.9677734375, auc: 0.95755892992
Step: 1320, loss: 0.0436512418091, accuracy: 0.9638671875, auc: 0.957581102848
Step: 1330, loss: 0.0386888682842, accuracy: 0.9697265625, auc: 0.95765465498
Step: 1350, loss: 0.0454179048538, accuracy: 0.95703125, auc: 0.957790315151
Step: 1360, loss: 0.0475404262543, accuracy: 0.9697265625, auc: 0.957867383957
Step: 1370, loss: 0.0437288023531, accuracy: 0.9638671875, auc: 0.957897365093
Step: 1380, loss: 0.0450800023973, accuracy: 0.9716796875, auc: 0.958040237427
Step: 1390, loss: 0.0470870956779, accuracy: 0.9638671875, auc: 0.958097457886
Step: 1400, loss: 0.0420254468918, accuracy: 0.9658203125, auc: 0.958227753639
Step: 1420, loss: 0.0374828353524, accuracy: 0.966796875, auc: 0.95837444067
Step: 1430, loss: 0.0404321327806, accuracy: 0.9697265625, auc: 0.958431720734
Step: 1440, loss: 0.0411650389433, accuracy: 0.962890625, auc: 0.95846170187
Step: 1450, loss: 0.0411781147122, accuracy: 0.96875, auc: 0.958517432213
Step: 1480, loss: 0.0416827201843, accuracy: 0.966796875, auc: 0.958842873573
Step: 1490, loss: 0.0455870628357, accuracy: 0.966796875, auc: 0.958874881268
Step: 1500, loss: 0.0389498695731, accuracy: 0.96484375, auc: 0.958923757076
Step: 1510, loss: 0.0376101769507, accuracy: 0.96875, auc: 0.958961844444
Step: 1520, loss: 0.0429160781205, accuracy: 0.9619140625, auc: 0.959059298038
Step: 1530, loss: 0.0353101342916, accuracy: 0.9658203125, auc: 0.95913118124
Step: 1540, loss: 0.0442190542817, accuracy: 0.966796875, auc: 0.959166884422
Step: 1550, loss: 0.0395638868213, accuracy: 0.9677734375, auc: 0.959216117859
Step: 1560, loss: 0.0394792929292, accuracy: 0.966796875, auc: 0.95925450325
Step: 1570, loss: 0.0412648431957, accuracy: 0.96484375, auc: 0.959353327751
Step: 1580, loss: 0.0379157811403, accuracy: 0.9638671875, auc: 0.959375441074
Step: 1590, loss: 0.0407469496131, accuracy: 0.9599609375, auc: 0.959423840046
Step: 1600, loss: 0.0371179617941, accuracy: 0.96875, auc: 0.959510564804
Step: 1620, loss: 0.0399380475283, accuracy: 0.9580078125, auc: 0.959638595581
Step: 1640, loss: 0.0406496115029, accuracy: 0.9697265625, auc: 0.959685862064
Step: 1650, loss: 0.0401513054967, accuracy: 0.96875, auc: 0.95977550745
Step: 1660, loss: 0.0424272343516, accuracy: 0.9638671875, auc: 0.959791660309
Step: 1670, loss: 0.0399652272463, accuracy: 0.9697265625, auc: 0.959845423698
Step: 1680, loss: 0.0395484939218, accuracy: 0.9658203125, auc: 0.959904134274
Step: 1690, loss: 0.0386858098209, accuracy: 0.9638671875, auc: 0.9599173069
Step: 1720, loss: 0.042072173208, accuracy: 0.9697265625, auc: 0.960128068924
Step: 1730, loss: 0.0402673520148, accuracy: 0.9580078125, auc: 0.96015638113
Step: 1740, loss: 0.0399186722934, accuracy: 0.9658203125, auc: 0.96017563343
Step: 1750, loss: 0.0382157340646, accuracy: 0.966796875, auc: 0.960241436958
Step: 1770, loss: 0.0436226427555, accuracy: 0.9599609375, auc: 0.960317075253
Step: 1790, loss: 0.0377070493996, accuracy: 0.96875, auc: 0.960407197475
Step: 1800, loss: 0.0395860821009, accuracy: 0.9658203125, auc: 0.960462331772
Step: 1810, loss: 0.0367764420807, accuracy: 0.9609375, auc: 0.960507750511
Step: 1820, loss: 0.0354869216681, accuracy: 0.9677734375, auc: 0.960539758205
Step: 1830, loss: 0.0376998260617, accuracy: 0.9658203125, auc: 0.960567593575
Step: 1850, loss: 0.0421861708164, accuracy: 0.96484375, auc: 0.960650980473
Step: 1890, loss: 0.037600710988, accuracy: 0.9619140625, auc: 0.960773050785
Step: 1900, loss: 0.0362802371383, accuracy: 0.95703125, auc: 0.96077388525
Step: 1920, loss: 0.0365306288004, accuracy: 0.970703125, auc: 0.96086037159
Step: 1990, loss: 0.0394714847207, accuracy: 0.96484375, auc: 0.961087465286
Step: 2080, loss: 0.0413855873048, accuracy: 0.966796875, auc: 0.961359381676
Step: 2350, loss: 0.033759932965, accuracy: 0.9677734375, auc: 0.962019920349
Step: 2380, loss: 0.0335225015879, accuracy: 0.9697265625, auc: 0.962081730366
Step: 2530, loss: 0.032491222024, accuracy: 0.96484375, auc: 0.962358593941
Step: 2600, loss: 0.0339137986302, accuracy: 0.96484375, auc: 0.962493121624
Step: 2620, loss: 0.0325324721634, accuracy: 0.9619140625, auc: 0.962534070015
Step: 2710, loss: 0.0290737524629, accuracy: 0.9609375, auc: 0.962691426277
Step: 2720, loss: 0.0330197252333, accuracy: 0.96875, auc: 0.962706804276
Step: 2800, loss: 0.0309434048831, accuracy: 0.95703125, auc: 0.962819457054
Step: 2810, loss: 0.032182764262, accuracy: 0.958984375, auc: 0.962834954262
Step: 2820, loss: 0.0335189923644, accuracy: 0.96875, auc: 0.962852776051
Step: 2850, loss: 0.033998593688, accuracy: 0.958984375, auc: 0.962920188904
Step: 2870, loss: 0.0294033773243, accuracy: 0.9609375, auc: 0.962926149368
Step: 2890, loss: 0.0345347486436, accuracy: 0.96875, auc: 0.96297711134
Step: 2910, loss: 0.0295092687011, accuracy: 0.962890625, auc: 0.962998747826
Step: 2930, loss: 0.033364918083, accuracy: 0.9599609375, auc: 0.963022530079
Step: 2940, loss: 0.0298673547804, accuracy: 0.9677734375, auc: 0.96304756403
Step: 2950, loss: 0.0301606822759, accuracy: 0.9599609375, auc: 0.963064789772
Step: 2970, loss: 0.029977960512, accuracy: 0.96484375, auc: 0.963064730167
Step: 2980, loss: 0.0318725369871, accuracy: 0.9619140625, auc: 0.963083744049
Step: 2990, loss: 0.0310963615775, accuracy: 0.9638671875, auc: 0.963111281395
Step: 3000, loss: 0.0296493843198, accuracy: 0.9599609375, auc: 0.963120043278
Step: 3010, loss: 0.03080860
2018-04-20 02:53:58.361855: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> 10.244.64.11:2222, 1 -> 10.244.192.14:2222}
2018-04-20 02:53:58.362155: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> 10.244.64.12:2222, 1 -> localhost:2222}
2018-04-20 02:53:58.364990: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:333] Started server with target: grpc://localhost:2222
WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.
Instructions for updating:
Use the retry module or similar alternatives.
WARNING:tensorflow:From dense_classifier.py:179: streaming_auc (from tensorflow.contrib.metrics.python.ops.metric_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.metrics.auc. Note that the order of the labels and predictions arguments has been switched.
WARNING:tensorflow:From dense_classifier.py:207: __init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.MonitoredTrainingSession
2018-04-20 02:54:18.452065: I tensorflow/core/distributed_runtime/master_session.cc:1033] Start master session 6b0b356be25fb16c with config: 
Step: 10, loss: 1.09713256359, accuracy: 0.7626953125, auc: 0.812998235226
Step: 20, loss: 0.653132736683, accuracy: 0.79296875, auc: 0.829907298088
Step: 40, loss: 0.520996928215, accuracy: 0.7939453125, auc: 0.843639492989
Step: 90, loss: 0.378034710884, accuracy: 0.833984375, auc: 0.858376204967
Step: 110, loss: 0.345747590065, accuracy: 0.8330078125, auc: 0.85918712616
Step: 140, loss: 0.334609806538, accuracy: 0.8388671875, auc: 0.862797379494
Step: 170, loss: 0.298130869865, accuracy: 0.8349609375, auc: 0.865898311138
Step: 200, loss: 0.285113930702, accuracy: 0.8427734375, auc: 0.869024276733
Step: 230, loss: 0.266689777374, accuracy: 0.8466796875, auc: 0.871098160744
Step: 280, loss: 0.254645496607, accuracy: 0.83984375, auc: 0.875074863434
Step: 300, loss: 0.247138291597, accuracy: 0.84375, auc: 0.877802073956
Step: 320, loss: 0.2435528934, accuracy: 0.8408203125, auc: 0.879171490669
Step: 350, loss: 0.227710917592, accuracy: 0.845703125, auc: 0.882143199444
Step: 380, loss: 0.223487049341, accuracy: 0.84375, auc: 0.884028553963
Step: 410, loss: 0.209981590509, accuracy: 0.8525390625, auc: 0.886577427387
Step: 420, loss: 0.214843586087, accuracy: 0.8359375, auc: 0.886981666088
Step: 430, loss: 0.203735426068, accuracy: 0.8447265625, auc: 0.887664079666
Step: 460, loss: 0.206396773458, accuracy: 0.833984375, auc: 0.889336228371
Step: 480, loss: 0.202137559652, accuracy: 0.8515625, auc: 0.890525996685
Step: 530, loss: 0.188785746694, accuracy: 0.8486328125, auc: 0.89306473732
Step: 570, loss: 0.17810857296, accuracy: 0.8486328125, auc: 0.894741296768
Step: 580, loss: 0.183210998774, accuracy: 0.845703125, auc: 0.895122766495
Step: 620, loss: 0.174709171057, accuracy: 0.845703125, auc: 0.896513998508
Step: 780, loss: 0.159651830792, accuracy: 0.8642578125, auc: 0.902012467384
Step: 800, loss: 0.166982114315, accuracy: 0.8662109375, auc: 0.902601182461
Step: 870, loss: 0.167030468583, accuracy: 0.8544921875, auc: 0.90428519249
Step: 880, loss: 0.160864323378, accuracy: 0.8662109375, auc: 0.90483212471
Step: 890, loss: 0.159607380629, accuracy: 0.8564453125, auc: 0.905015647411
Step: 970, loss: 0.156185895205, accuracy: 0.8623046875, auc: 0.906433522701
Step: 1020, loss: 0.148089289665, accuracy: 0.8564453125, auc: 0.907455146313
Step: 1140, loss: 0.144499853253, accuracy: 0.861328125, auc: 0.909742474556
Step: 1180, loss: 0.140416711569, accuracy: 0.84765625, auc: 0.910388529301
Step: 1240, loss: 0.139100909233, accuracy: 0.8583984375, auc: 0.911446273327
Step: 1250, loss: 0.128061816096, accuracy: 0.8642578125, auc: 0.91159504652
Step: 1270, loss: 0.136243879795, accuracy: 0.8759765625, auc: 0.911804914474
Step: 1400, loss: 0.132757976651, accuracy: 0.857421875, auc: 0.913594782352
Step: 1470, loss: 0.124085471034, accuracy: 0.86328125, auc: 0.914578199387
Step: 1580, loss: 0.124271832407, accuracy: 0.861328125, auc: 0.915803253651
Step: 1600, loss: 0.120072171092, accuracy: 0.8662109375, auc: 0.916062116623
Step: 1620, loss: 0.120995521545, accuracy: 0.8642578125, auc: 0.91643255949
Step: 1640, loss: 0.120596952736, accuracy: 0.865234375, auc: 0.916652917862
Step: 1660, loss: 0.116640746593, accuracy: 0.861328125, auc: 0.916840553284
Step: 1680, loss: 0.126433640718, accuracy: 0.8603515625, auc: 0.917049109936
Step: 1690, loss: 0.11717518419, accuracy: 0.85546875, auc: 0.917146861553
Step: 1710, loss: 0.122429721057, accuracy: 0.86328125, auc: 0.917358577251
Step: 1720, loss: 0.118816271424, accuracy: 0.86328125, auc: 0.917464435101
Step: 1750, loss: 0.119506217539, accuracy: 0.859375, auc: 0.917728126049
Step: 1760, loss: 0.115253329277, accuracy: 0.8662109375, auc: 0.917863965034
Step: 1800, loss: 0.118245407939, accuracy: 0.8564453125, auc: 0.918234586716
Step: 1820, loss: 0.116227716208, accuracy: 0.8544921875, auc: 0.918386518955
Step: 1840, loss: 0.118843503296, accuracy: 0.86328125, auc: 0.918551743031
Step: 1860, loss: 0.110447630286, accuracy: 0.861328125, auc: 0.918767571449
Step: 1880, loss: 0.110214918852, accuracy: 0.8681640625, auc: 0.918940365314
Step: 1890, loss: 0.112888090312, accuracy: 0.85546875, auc: 0.919033169746
Step: 1900, loss: 0.113152742386, accuracy: 0.859375, auc: 0.91910892725
Step: 1910, loss: 0.113328114152, accuracy: 0.86328125, auc: 0.919212639332
Step: 1930, loss: 0.110835775733, accuracy: 0.865234375, auc: 0.919378995895
Step: 1950, loss: 0.105923727155, accuracy: 0.861328125, auc: 0.91955935955
Step: 1960, loss: 0.108811169863, accuracy: 0.8623046875, auc: 0.919652819633
Step: 1980, loss: 0.111318081617, accuracy: 0.8525390625, auc: 0.919858574867
Step: 2000, loss: 0.112941466272, accuracy: 0.8662109375, auc: 0.920047163963
Step: 2020, loss: 0.10676407814, accuracy: 0.8720703125, auc: 0.920164644718
Step: 2030, loss: 0.111018955708, accuracy: 0.8671875, auc: 0.920260190964
Step: 2080, loss: 0.114006556571, accuracy: 0.869140625, auc: 0.920584082603
Step: 2090, loss: 0.109182998538, accuracy: 0.8583984375, auc: 0.920641064644
Step: 2110, loss: 0.105653956532, accuracy: 0.876953125, auc: 0.920837640762
Step: 2130, loss: 0.103913784027, accuracy: 0.8720703125, auc: 0.9209446311
Step: 2150, loss: 0.102406241, accuracy: 0.861328125, auc: 0.921084463596
Step: 2160, loss: 0.103311628103, accuracy: 0.86328125, auc: 0.921150505543
Step: 2170, loss: 0.101104557514, accuracy: 0.8701171875, auc: 0.921322524548
Step: 2200, loss: 0.104142539203, accuracy: 0.8681640625, auc: 0.921501815319
Step: 2210, loss: 0.104649938643, accuracy: 0.8671875, auc: 0.921573579311
Step: 2260, loss: 0.105122715235, accuracy: 0.8681640625, auc: 0.921896159649
Step: 2270, loss: 0.0983881428838, accuracy: 0.8623046875, auc: 0.92192530632
Step: 2280, loss: 0.105525672436, accuracy: 0.8681640625, auc: 0.922011494637
Step: 2300, loss: 0.100174725056, accuracy: 0.880859375, auc: 0.922149717808
Step: 2310, loss: 0.100090578198, accuracy: 0.8603515625, auc: 0.922215104103
Step: 2340, loss: 0.104663074017, accuracy: 0.865234375, auc: 0.92240780592
Step: 2350, loss: 0.101491361856, accuracy: 0.8701171875, auc: 0.922483682632
Step: 2370, loss: 0.100263744593, accuracy: 0.8681640625, auc: 0.922675669193
Step: 2390, loss: 0.0987630411983, accuracy: 0.86328125, auc: 0.922798991203
Step: 2400, loss: 0.10188537091, accuracy: 0.87109375, auc: 0.922863483429
Step: 2420, loss: 0.100553922355, accuracy: 0.8642578125, auc: 0.922999382019
Step: 2440, loss: 0.0972376614809, accuracy: 0.8681640625, auc: 0.923084199429
Step: 2450, loss: 0.0993950292468, accuracy: 0.87109375, auc: 0.923162996769
Step: 2460, loss: 0.103423804045, accuracy: 0.869140625, auc: 0.923217773438
Step: 2500, loss: 0.0953431725502, accuracy: 0.8623046875, auc: 0.923400461674
Step: 2520, loss: 0.0999698713422, accuracy: 0.8642578125, auc: 0.923497140408
Step: 2540, loss: 0.0960265994072, accuracy: 0.8720703125, auc: 0.923618078232
Step: 2560, loss: 0.0981454178691, accuracy: 0.8720703125, auc: 0.923843741417
Step: 2580, loss: 0.09555221349, accuracy: 0.8701171875, auc: 0.923920750618
Step: 2600, loss: 0.0952171310782, accuracy: 0.8583984375, auc: 0.924014508724
Step: 2610, loss: 0.0958336144686, accuracy: 0.86328125, auc: 0.924046039581
Step: 2620, loss: 0.0922059565783, accuracy: 0.875, auc: 0.924117267132
Step: 2630, loss: 0.0963149294257, accuracy: 0.8671875, auc: 0.924183249474
Step: 2660, loss: 0.0970893353224, accuracy: 0.8642578125, auc: 0.924361646175
Step: 2670, loss: 0.0971112176776, accuracy: 0.865234375, auc: 0.924421906471
Step: 2680, loss: 0.0983989164233, accuracy: 0.87890625, auc: 0.924512088299
Step: 2700, loss: 0.0923418402672, accuracy: 0.8662109375, auc: 0.924602031708
Step: 2710, loss: 0.0917731225491, accuracy: 0.8642578125, auc: 0.92464029789
Step: 2720, loss: 0.0917548984289, accuracy: 0.87109375, auc: 0.924710452557
Step: 2730, loss: 0.0944230780005, accuracy: 0.8603515625, auc: 0.924739599228
Step: 2740, loss: 0.0955622196198, accuracy: 0.8662109375, auc: 0.92484742403
Step: 2750, loss: 0.0903424248099, accuracy: 0.9072265625, auc: 0.924899399281
Step: 2790, loss: 0.0892720744014, accuracy: 0.91015625, auc: 0.925091743469
Step: 2800, loss: 0.0947996675968, accuracy: 0.900390625, auc: 0.925120949745
Step: 2810, loss: 0.0885417610407, accuracy: 0.9169921875, auc: 0.92522162199
Step: 2820, loss: 0.0874149128795, accuracy: 0.90234375, auc: 0.925257027149
Step: 2830, loss: 0.0857313722372, accuracy: 0.908203125, auc: 0.925302803516
Step: 2840, loss: 0.0889687091112, accuracy: 0.904296875, auc: 0.92534160614
Step: 2850, loss: 0.0917082205415, accuracy: 0.9033203125, auc: 0.925437271595
Step: 2860, loss: 0.0896929726005, accuracy: 0.9111328125, auc: 0.925511538982
Step: 2870, loss: 0.0820901021361, accuracy: 0.912109375, auc: 0.925582885742
Step: 2880, loss: 0.0907168909907, accuracy: 0.904296875, auc: 0.925633192062
Step: 2900, loss: 0.0905193537474, accuracy: 0.90625, auc: 0.925726473331
Step: 2910, loss: 0.0859118402004, accuracy: 0.90625, auc: 0.925765931606
Step: 2920, loss: 0.0859459042549, accuracy: 0.9091796875, auc: 0.925837874413
Step: 2930, loss: 0.0846454948187, accuracy: 0.91015625, auc: 0.925953984261
Step: 2940, loss: 0.0870019346476, accuracy: 0.9072265625, auc: 0.926001012325
Step: 2950, loss: 0.0880602449179, accuracy: 0.90234375, auc: 0.926040887833
Step: 2960, loss: 0.0884352102876, accuracy: 0.9072265625, auc: 0.92610347271
Step: 2970, loss: 0.0935485064983, accuracy: 0.912109375, auc: 0.926156699657
Step: 2980, loss: 0.0846024006605, accuracy: 0.90625, auc: 0.926225423813
Step: 2990, loss: 0.0883344635367, accuracy: 0.90625, auc: 0.926286756992
Step: 3000, loss: 0.0892874896526, accuracy: 0.9013671875, auc: 0.92631316185
Step: 3030, loss: 0.0910678952932, accuracy: 0.908203125, auc: 0.926517367363
Step: 3040, loss: 0.0892729908228, accuracy: 0.9169921875, auc: 0.926612079144
Step: 3060, loss: 0.0905537605286, accuracy: 0.9013671875, auc: 0.926705300808
Step: 3070, loss: 0.0836839228868, accuracy: 0.9033203125, auc: 0.926733613014
Step: 3080, loss: 0.0871620476246, accuracy: 0.9130859375, auc: 0.926805198193
Step: 3090, loss: 0.0883719995618, accuracy: 0.9091796875, auc: 0.926875174046
Step: 3100, loss: 0.0875856876373, accuracy: 0.9072265625, auc: 0.926921308041
Step: 3130, loss: 0.0851136744022, accuracy: 0.9052734375, auc: 0.927082657814
Step: 3160, loss: 0.0850699394941, accuracy: 0.9052734375, auc: 0.927204966545
Step: 3170, loss: 0.0851431787014, accuracy: 0.904296875, auc: 0.92726546526
Step: 3210, loss: 0.0850224643946, accuracy: 0.91015625, auc: 0.927471935749
Step: 3220, loss: 0.0855458825827, accuracy: 0.90625, auc: 0.927529573441
Step: 3250, loss: 0.0857986658812, accuracy: 0.90625, auc: 0.927727639675
Step: 3260, loss: 0.0857250839472, accuracy: 0.908203125, auc: 0.92777121067
Step: 3270, loss: 0.082896143198, accuracy: 0.9150390625, auc: 0.927849888802
Step: 3280, loss: 0.0850668549538, accuracy: 0.8984375, auc: 0.927889287472
Step: 3300, loss: 0.0794908702374, accuracy: 0.912109375, auc: 0.927997589111
Step: 3310, loss: 0.0890547186136, accuracy: 0.904296875, auc: 0.928112089634
Step: 3330, loss: 0.0874366611242, accuracy: 0.91015625, auc: 0.928230762482
Step: 3340, loss: 0.0836169496179, accuracy: 0.908203125, auc: 0.928284049034
Step: 3360, loss: 0.0827002078295, accuracy: 0.9072265625, auc: 0.92839974165
Step: 3370, loss: 0.0834207385778, accuracy: 0.9072265625, auc: 0.928451418877
Step: 3380, loss: 0.081010133028, accuracy: 0.904296875, auc: 0.928504824638
Step: 3390, loss: 0.0857841074467, accuracy: 0.912109375, auc: 0.928572952747
Step: 3400, loss: 0.0838656723499, accuracy: 0.9052734375, auc: 0.928624689579
Step: 3430, loss: 0.0807197317481, accuracy: 0.90625, auc: 0.928780436516
Step: 3440, loss: 0.0777591913939, accuracy: 0.9052734375, auc: 0.928822755814
Step: 3450, loss: 0.0786270797253, accuracy: 0.9072265625, auc: 0.928875207901
Step: 3460, loss: 0.0825766995549, accuracy: 0.9111328125, auc: 0.928945481777
Step: 3500, loss: 0.0840214490891, accuracy: 0.9072265625, auc: 0.929175078869
Step: 3510, loss: 0.083333902061, accuracy: 0.90625, auc: 0.929258406162
Step: 3520, loss: 0.0792204141617, accuracy: 0.9072265625, auc: 0.929301381111
Step: 3530, loss: 0.0829411000013, accuracy: 0.904296875, auc: 0.929422736168
Step: 3540, loss: 0.0815858617425, accuracy: 0.9072265625, auc: 0.92947602272
Step: 3550, loss: 0.0837749838829, accuracy
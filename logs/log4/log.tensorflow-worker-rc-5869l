2018-04-20 02:53:49.120327: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> 10.244.64.11:2222, 1 -> 10.244.192.14:2222}
2018-04-20 02:53:49.120500: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2222, 1 -> 10.244.192.15:2222}
2018-04-20 02:53:49.121855: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:333] Started server with target: grpc://localhost:2222
WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.
Instructions for updating:
Use the retry module or similar alternatives.
WARNING:tensorflow:From dense_classifier.py:179: streaming_auc (from tensorflow.contrib.metrics.python.ops.metric_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.metrics.auc. Note that the order of the labels and predictions arguments has been switched.
WARNING:tensorflow:From dense_classifier.py:207: __init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.MonitoredTrainingSession
2018-04-20 02:54:03.201777: I tensorflow/core/distributed_runtime/master_session.cc:1033] Start master session ee1d3975492a2198 with config: 
Step: 0, loss: 32.4433517456, accuracy: 0.3330078125, auc: 0.327845931053
Step: 30, loss: 0.610340952873, accuracy: 0.794921875, auc: 0.839033603668
Step: 50, loss: 0.469680130482, accuracy: 0.79296875, auc: 0.846512973309
Step: 60, loss: 0.426676988602, accuracy: 0.8046875, auc: 0.850128114223
Step: 70, loss: 0.4422544837, accuracy: 0.8046875, auc: 0.852396845818
Step: 80, loss: 0.387028992176, accuracy: 0.82421875, auc: 0.854054987431
Step: 90, loss: 0.370633304119, accuracy: 0.8408203125, auc: 0.857212543488
Step: 100, loss: 0.370369315147, accuracy: 0.8271484375, auc: 0.85815435648
Step: 120, loss: 0.353132396936, accuracy: 0.8408203125, auc: 0.860823214054
Step: 130, loss: 0.351013660431, accuracy: 0.8310546875, auc: 0.861501097679
Step: 150, loss: 0.316683918238, accuracy: 0.8408203125, auc: 0.863758444786
Step: 160, loss: 0.30491271615, accuracy: 0.8427734375, auc: 0.864580154419
Step: 180, loss: 0.29604613781, accuracy: 0.83984375, auc: 0.867123126984
Step: 190, loss: 0.286398291588, accuracy: 0.8408203125, auc: 0.868058741093
Step: 210, loss: 0.278212040663, accuracy: 0.8486328125, auc: 0.869728505611
Step: 220, loss: 0.281734347343, accuracy: 0.833984375, auc: 0.870248198509
Step: 240, loss: 0.287551701069, accuracy: 0.8408203125, auc: 0.872057914734
Step: 250, loss: 0.253502190113, accuracy: 0.8515625, auc: 0.872865140438
Step: 260, loss: 0.270038694143, accuracy: 0.841796875, auc: 0.873900294304
Step: 270, loss: 0.251367270947, accuracy: 0.8388671875, auc: 0.874467968941
Step: 290, loss: 0.259473621845, accuracy: 0.8359375, auc: 0.875816643238
Step: 300, loss: 0.231071174145, accuracy: 0.845703125, auc: 0.87688678503
Step: 310, loss: 0.23977035284, accuracy: 0.8408203125, auc: 0.878456175327
Step: 330, loss: 0.226587086916, accuracy: 0.8486328125, auc: 0.880005955696
Step: 340, loss: 0.241554707289, accuracy: 0.853515625, auc: 0.881343364716
Step: 360, loss: 0.221196249127, accuracy: 0.8359375, auc: 0.882753431797
Step: 370, loss: 0.230980873108, accuracy: 0.8466796875, auc: 0.883487939835
Step: 390, loss: 0.214240476489, accuracy: 0.8427734375, auc: 0.884655296803
Step: 400, loss: 0.226730853319, accuracy: 0.845703125, auc: 0.885188341141
Step: 410, loss: 0.217956259847, accuracy: 0.8515625, auc: 0.885839641094
Step: 440, loss: 0.206900268793, accuracy: 0.83984375, auc: 0.888209462166
Step: 450, loss: 0.195002675056, accuracy: 0.837890625, auc: 0.888756394386
Step: 470, loss: 0.20522364974, accuracy: 0.8525390625, auc: 0.889987468719
Step: 490, loss: 0.191115692258, accuracy: 0.837890625, auc: 0.890776753426
Step: 500, loss: 0.199678301811, accuracy: 0.849609375, auc: 0.891310811043
Step: 510, loss: 0.192385151982, accuracy: 0.8515625, auc: 0.891770899296
Step: 520, loss: 0.192146882415, accuracy: 0.8349609375, auc: 0.892644286156
Step: 540, loss: 0.18342833221, accuracy: 0.8544921875, auc: 0.89354878664
Step: 550, loss: 0.195636868477, accuracy: 0.8466796875, auc: 0.893981635571
Step: 560, loss: 0.188594430685, accuracy: 0.8515625, auc: 0.894383609295
Step: 590, loss: 0.18447907269, accuracy: 0.84375, auc: 0.895495414734
Step: 600, loss: 0.175327181816, accuracy: 0.8486328125, auc: 0.895866155624
Step: 610, loss: 0.180560424924, accuracy: 0.8525390625, auc: 0.896234750748
Step: 630, loss: 0.17490452528, accuracy: 0.853515625, auc: 0.896924376488
Step: 640, loss: 0.17810189724, accuracy: 0.8671875, auc: 0.897387742996
Step: 650, loss: 0.173026800156, accuracy: 0.849609375, auc: 0.897626519203
Step: 660, loss: 0.176480442286, accuracy: 0.86328125, auc: 0.897977471352
Step: 670, loss: 0.170571804047, accuracy: 0.8623046875, auc: 0.898295402527
Step: 680, loss: 0.164172708988, accuracy: 0.8623046875, auc: 0.898713052273
Step: 690, loss: 0.165263980627, accuracy: 0.8671875, auc: 0.899127602577
Step: 700, loss: 0.171591103077, accuracy: 0.861328125, auc: 0.899674117565
Step: 710, loss: 0.164083734155, accuracy: 0.865234375, auc: 0.899966001511
Step: 720, loss: 0.169577732682, accuracy: 0.8603515625, auc: 0.900297284126
Step: 730, loss: 0.166937917471, accuracy: 0.8466796875, auc: 0.900563120842
Step: 740, loss: 0.162827506661, accuracy: 0.853515625, auc: 0.900798678398
Step: 750, loss: 0.163918092847, accuracy: 0.8671875, auc: 0.901162028313
Step: 760, loss: 0.161585003138, accuracy: 0.86328125, auc: 0.901465654373
Step: 770, loss: 0.169523730874, accuracy: 0.8603515625, auc: 0.901742815971
Step: 790, loss: 0.166447877884, accuracy: 0.85546875, auc: 0.902251303196
Step: 810, loss: 0.157742738724, accuracy: 0.86328125, auc: 0.902890443802
Step: 820, loss: 0.160733088851, accuracy: 0.87109375, auc: 0.903127789497
Step: 830, loss: 0.162633284926, accuracy: 0.85546875, auc: 0.903351902962
Step: 840, loss: 0.155604988337, accuracy: 0.8671875, auc: 0.903668761253
Step: 850, loss: 0.152496024966, accuracy: 0.8583984375, auc: 0.903846681118
Step: 860, loss: 0.159856855869, accuracy: 0.8583984375, auc: 0.904090225697
Step: 910, loss: 0.150828063488, accuracy: 0.85546875, auc: 0.905226528645
Step: 920, loss: 0.153692364693, accuracy: 0.8486328125, auc: 0.905367910862
Step: 930, loss: 0.154688924551, accuracy: 0.8623046875, auc: 0.905615091324
Step: 940, loss: 0.153389722109, accuracy: 0.8671875, auc: 0.905858099461
Step: 950, loss: 0.158443748951, accuracy: 0.8681640625, auc: 0.906069755554
Step: 960, loss: 0.15349650383, accuracy: 0.8515625, auc: 0.906253993511
Step: 980, loss: 0.157388269901, accuracy: 0.857421875, auc: 0.906617283821
Step: 990, loss: 0.151623800397, accuracy: 0.8564453125, auc: 0.906803965569
Step: 1000, loss: 0.15457560122, accuracy: 0.859375, auc: 0.907036006451
Step: 1010, loss: 0.155867531896, accuracy: 0.853515625, auc: 0.907217562199
Step: 1030, loss: 0.147060900927, accuracy: 0.8544921875, auc: 0.90761423111
Step: 1040, loss: 0.149040043354, accuracy: 0.85546875, auc: 0.907818436623
Step: 1050, loss: 0.150390863419, accuracy: 0.8525390625, auc: 0.907955169678
Step: 1060, loss: 0.149301633239, accuracy: 0.8525390625, auc: 0.908314287663
Step: 1070, loss: 0.149713903666, accuracy: 0.8525390625, auc: 0.908479869366
Step: 1080, loss: 0.148740321398, accuracy: 0.859375, auc: 0.908684074879
Step: 1090, loss: 0.146476924419, accuracy: 0.8515625, auc: 0.908855736256
Step: 1100, loss: 0.140690222383, accuracy: 0.861328125, auc: 0.909082114697
Step: 1110, loss: 0.148777395487, accuracy: 0.84765625, auc: 0.909226834774
Step: 1120, loss: 0.148301750422, accuracy: 0.84765625, auc: 0.909367322922
Step: 1130, loss: 0.13913539052, accuracy: 0.8564453125, auc: 0.909533143044
Step: 1150, loss: 0.15019595623, accuracy: 0.859375, auc: 0.909928262234
Step: 1160, loss: 0.136735767126, accuracy: 0.853515625, auc: 0.910110712051
Step: 1170, loss: 0.146896377206, accuracy: 0.8544921875, auc: 0.910281538963
Step: 1190, loss: 0.13804858923, accuracy: 0.84765625, auc: 0.910500049591
Step: 1200, loss: 0.14362770319, accuracy: 0.86328125, auc: 0.910691797733
Step: 1210, loss: 0.138879179955, accuracy: 0.8505859375, auc: 0.910838663578
Step: 1220, loss: 0.142387568951, accuracy: 0.8544921875, auc: 0.910994529724
Step: 1230, loss: 0.141497805715, accuracy: 0.8564453125, auc: 0.91114372015
Step: 1280, loss: 0.142730921507, accuracy: 0.8662109375, auc: 0.911986470222
Step: 1290, loss: 0.139357343316, accuracy: 0.8681640625, auc: 0.912110328674
Step: 1300, loss: 0.130841583014, accuracy: 0.8564453125, auc: 0.912241816521
Step: 1310, loss: 0.139750629663, accuracy: 0.8603515625, auc: 0.912365913391
Step: 1320, loss: 0.142001569271, accuracy: 0.873046875, auc: 0.912537693977
Step: 1330, loss: 0.13406842947, accuracy: 0.8671875, auc: 0.912651121616
Step: 1340, loss: 0.137980133295, accuracy: 0.873046875, auc: 0.912864685059
Step: 1350, loss: 0.129822492599, accuracy: 0.859375, auc: 0.912912666798
Step: 1360, loss: 0.137836486101, accuracy: 0.8642578125, auc: 0.913036823273
Step: 1370, loss: 0.132370352745, accuracy: 0.8759765625, auc: 0.913208067417
Step: 1380, loss: 0.137384384871, accuracy: 0.8720703125, auc: 0.913355469704
Step: 1390, loss: 0.12999022007, accuracy: 0.8603515625, auc: 0.913498878479
Step: 1410, loss: 0.131081551313, accuracy: 0.86328125, auc: 0.913672029972
Step: 1420, loss: 0.132482782006, accuracy: 0.8701171875, auc: 0.913946926594
Step: 1430, loss: 0.136017993093, accuracy: 0.869140625, auc: 0.914081633091
Step: 1440, loss: 0.126374185085, accuracy: 0.86328125, auc: 0.914238452911
Step: 1450, loss: 0.130794167519, accuracy: 0.865234375, auc: 0.914330363274
Step: 1460, loss: 0.129331916571, accuracy: 0.869140625, auc: 0.914472162724
Step: 1480, loss: 0.12625670433, accuracy: 0.876953125, auc: 0.914779961109
Step: 1490, loss: 0.132484972477, accuracy: 0.85546875, auc: 0.914871275425
Step: 1500, loss: 0.124616101384, accuracy: 0.85546875, auc: 0.914959013462
Step: 1510, loss: 0.120127931237, accuracy: 0.8701171875, auc: 0.915100753307
Step: 1530, loss: 0.122103445232, accuracy: 0.8720703125, auc: 0.915202558041
Step: 1540, loss: 0.123383432627, accuracy: 0.87109375, auc: 0.915369749069
Step: 1550, loss: 0.122421786189, accuracy: 0.8505859375, auc: 0.915452778339
Step: 1560, loss: 0.125187009573, accuracy: 0.86328125, auc: 0.915601313114
Step: 1570, loss: 0.119755081832, accuracy: 0.861328125, auc: 0.915715038776
Step: 1590, loss: 0.125617653131, accuracy: 0.8740234375, auc: 0.915927648544
Step: 1610, loss: 0.11991904676, accuracy: 0.8515625, auc: 0.916292190552
Step: 1630, loss: 0.118999361992, accuracy: 0.8701171875, auc: 0.916527748108
Step: 1650, loss: 0.121759414673, accuracy: 0.8740234375, auc: 0.916748523712
Step: 1670, loss: 0.115962952375, accuracy: 0.8583984375, auc: 0.916963934898
Step: 1700, loss: 0.118062503636, accuracy: 0.8603515625, auc: 0.917250394821
Step: 1730, loss: 0.115247547626, accuracy: 0.8603515625, auc: 0.917554795742
Step: 1740, loss: 0.12104831636, accuracy: 0.861328125, auc: 0.917637825012
Step: 1780, loss: 0.118266545236, accuracy: 0.861328125, auc: 0.917971551418
Step: 1790, loss: 0.115684740245, accuracy: 0.859375, auc: 0.918062031269
Step: 1810, loss: 0.115119837224, accuracy: 0.857421875, auc: 0.918311715126
Step: 1830, loss: 0.116873174906, accuracy: 0.857421875, auc: 0.918457210064
Step: 1850, loss: 0.116114154458, accuracy: 0.875, auc: 0.918681383133
Step: 1870, loss: 0.112648569047, accuracy: 0.857421875, auc: 0.918843567371
Step: 1920, loss: 0.109810397029, accuracy: 0.8525390625, auc: 0.919298827648
Step: 1940, loss: 0.118426017463, accuracy: 0.865234375, auc: 0.919467628002
Step: 1970, loss: 0.108295485377, accuracy: 0.8623046875, auc: 0.919749677181
Step: 1990, loss: 0.106980599463, accuracy: 0.8671875, auc: 0.919944286346
Step: 2010, loss: 0.110017538071, accuracy: 0.8583984375, auc: 0.920091867447
Step: 2040, loss: 0.111369878054, accuracy: 0.876953125, auc: 0.920353353024
Step: 2060, loss: 0.110555395484, accuracy: 0.8662109375, auc: 0.920444071293
Step: 2070, loss: 0.107672952116, accuracy: 0.86328125, auc: 0.920516133308
Step: 2100, loss: 0.108928918839, accuracy: 0.869140625, auc: 0.920729339123
Step: 2120, loss: 0.103767409921, accuracy: 0.861328125, auc: 0.920873701572
Step: 2140, loss: 0.105996057391, accuracy: 0.875, auc: 0.921030640602
Step: 2180, loss: 0.103073485196, accuracy: 0.8603515625, auc: 0.921386778355
Step: 2190, loss: 0.106148436666, accuracy: 0.8642578125, auc: 0.92144036293
Step: 2220, loss: 0.104974105954, accuracy: 0.876953125, auc: 0.921671509743
Step: 2240, loss: 0.0991664677858, accuracy: 0.87109375, auc: 0.921772778034
Step: 2250, loss: 0.105825647712, accuracy: 0.8623046875, auc: 0.921819865704
Step: 2290, loss: 0.101751752198, accuracy: 0.869140625, auc: 0.922070741653
Step: 2320, loss: 0.0953359529376, accuracy: 0.86328125, auc: 0.922276377678
Step: 2330, loss: 0.0981922969222, accuracy: 0.87890625, auc: 0.922363698483
Step: 2360, loss: 0.101548060775, accuracy: 0.861328125, auc: 0.922614991665
Step: 2380, loss: 0.0963391363621, accuracy: 0.87109375, auc: 0.922731101513
Step: 2410, loss: 0.10098002851, accuracy: 0.8720703125, auc: 0.922942280769
Step: 2430, loss: 0.0997053086758, accuracy: 0.8662109375, auc: 0.923040568829
Step: 2480, loss: 0.10033878684, accuracy: 0.86328125, auc: 0.923294425011
Step: 2490, loss: 0.099672883749, accuracy: 0.865234375, auc: 0.923347353935
Step: 2510, loss: 0.100568465889, accuracy: 0.869140625, auc: 0.923452615738
Step: 2530, loss: 0.1002